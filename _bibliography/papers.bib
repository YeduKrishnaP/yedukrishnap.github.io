---
---

@ARTICLE{11293968,
  author={Balasubramanian, S. and Yedu Krishna, P. and Sai Sriram, Talasu and Sai Subramaniam, M. and Pranav Phanindra Sai, Manepalli and Mukkamala, Ravi},
  journal={IEEE Access}, 
  title={S2IL: Structurally Stable Incremental Learning}, 
  year={2025},
  volume={13},
  number={},
  abstract={Feature Distillation (FD) strategies are proven to be effective in mitigating Catastrophic Forgetting (CF) seen in Class Incremental Learning (CIL). However, current FD approaches enforce strict alignment of feature magnitudes and directions across incremental steps, limiting the model’s ability to adapt to new knowledge. In this paper, we propose Structurally Stable Incremental Learning (S²IL), a FD method for CIL that mitigates forgetting by focusing on preserving the overall spatial patterns of features which promote flexible (plasticity) yet stable representations that preserve old knowledge (stability). We also demonstrate that our proposed method S2IL achieves strong incremental accuracy and outperforms other FD methods on SOTA benchmark datasets CIFAR-100, ImageNet-100 and ImageNet-1K. Notably, S²IL outperforms other methods by a significant margin in scenarios that have a large number of incremental tasks. The source code is available at https://github.com/dlclub2311/Structurally-Stable-Incremental-Learning},
  pages={210237-210245},
  keywords={Adaptation models;Incremental learning;Stability criteria;Training;Data models;Indexes;Upper bound;Thermal stability;Minimization;Limiting;Catastrophic forgetting;class incremental learning;feature distillation;plasticity;stability;structural similarity},
  doi={10.1109/ACCESS.2025.3642464},
  pdf={assets/pdf/S2IL_Structurally_Stable_Incremental_Learning.pdf},
  code={https://github.com/dlclub2311/Structurally-Stable-Incremental-Learning}
}


@inproceedings{10.1145/3702250.3702267,
author = {S, Balasubramanian and M, Sai Subramaniam and Talasu, Sai Sriram and Sai, Manepalli Pranav Phanindra and P, Yedu Krishna and Gera, Darshan and Mukkamala, Ravi},
title = {EXACFS - A CIL Method to Mitigate Catastrophic Forgetting},
year = {2025},
isbn = {9798400710759},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702250.3702267},
doi = {10.1145/3702250.3702267},
abstract = {Deep neural networks (DNNs) excel at learning from static datasets but struggle with continual learning, where data arrives sequentially. Catastrophic forgetting, the phenomenon of forgetting previously learned knowledge, is a primary challenge. This paper introduces EXponentially Averaged Class-wise Feature Significance (EXACFS) to mitigate this issue in the class incremental learning (CIL) setting. By estimating the significance of model features for each learned class using loss gradients, gradually aging the significance through the incremental tasks and preserving the significant features through a distillation loss, EXACFS effectively balances remembering old knowledge (stability) and learning new knowledge (plasticity). Extensive experiments on CIFAR-100 and ImageNet-100 demonstrate EXACFS’s superior performance in preserving stability while acquiring plasticity.},
booktitle = {Proceedings of the Fifteenth Indian Conference on Computer Vision Graphics and Image Processing},
articleno = {17},
numpages = {8},
keywords = {Class Incremental Learning, Feature Distillation, Catastrophic Forgetting, Stability, Plasticity},
location = {},
series = {ICVGIP '24},
pdf={assets/pdf/EXACFS.pdf},
}

@misc{balasubramanian2024classifyvistawceclassificationvisualunderstanding,
      title={ClassifyViStA:WCE Classification with Visual understanding through Segmentation and Attention}, 
      author={S. Balasubramanian and Ammu Abhishek and Yedu Krishna and Darshan Gera},
      year={2024},
      eprint={2412.18591},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.18591},
      pdf={assets/pdf/ClassifyVista.pdf},
      code={https://github.com/1980x/WCEClassifyViStA},

}